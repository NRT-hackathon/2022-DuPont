# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zAyiq6cN_OEIjv9yaG6xJXbop2lcREhP
"""
#%%
# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import scipy
# from sklearn.linear_model import ElasticNet, Ridge, Lasso, LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.neighbors import KNeighborsRegressor
# from sklearn.ensemble import RandomForestRegressor
from xgboost.sklearn import XGBRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, DotProduct, Matern, RationalQuadratic, WhiteKernel
from sklearn.model_selection import GridSearchCV, KFold
# from sklearn.preprocessing import StandardScaler
import pickle
#%%
# Load dataset
Dataset = pd.read_csv("data/UD_867_formulation_training.csv")
TARGETS = ['Water_Absorption_%','Hardness','Thermal_Conductivity_(mW/m.K)']
NR_FEATURES = ['Clarifier_1','Clarifier_2','Clarifier_3',
                'Polymer_3','UV_absorber_1','UV_absorber_2',
                'Filler_2','Filler_3']
TRAINING_OPTION = 'partial' # if remove 'partial'
#%%
"""# 1. Data preprocessing"""

# Reproducibility
SEED = 12345
# Select only features from dataset
X_train = Dataset.drop(columns=['name']+TARGETS)
# add entropy as a feature
X_train['entropy']=scipy.stats.entropy(X_train, axis=1)
# removing some features may help?
if TRAINING_OPTION == 'partial':
    X_train = X_train.drop(columns=NR_FEATURES)
# Select only targets from dataset
Y_train = Dataset[TARGETS]
# Standardization
mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0)
X_train = (X_train - mu)/sigma
# y_train = Y_train[TARGETS[0]]
#%%
"""# 2. Models definitions"""

models =    {
    # Similarity-based regressors
    'KNeighborsRegressor':{
            # 'n_neighbors': np.arange(1,3,2),
            'n_neighbors': np.arange(1,15,2),
            'weights': ['uniform','distance'],
            'p': [1,2],
            'metric': ['minkowski','chebyshev'],
    },
    'GaussianProcessRegressor':{
            'kernel':None,
            'n_restarts_optimizer':[5],
            'random_state':[SEED]
    },
    # Tree-based regressors
    'XGBRegressor':{
            'learning_rate': np.arange(0.025,0.150,0.025),
            'gamma':np.arange(0.05,0.45,0.05),
            # 'max_depth':np.arange(2,14,2),
            # 'min_child_weight':np.arange(1,8,1),
            'n_estimators':np.arange(10,80,5),
            # 'subsample':np.arange(0.60,0.95,0.05),
            # 'colsample_bytree':np.arange(0.60,0.95,0.05),
            'reg_alpha':np.logspace(-3,2,6), #alpha
            'reg_lambda':np.logspace(-3,2,6),#lambda
    },
            }
#%%
# Model instantiation 
def set_model(name):
    """Initialization module for models to be evaluated"""
    # Similarity-based regressors
    if name=='KNeighborsRegressor':
        model = KNeighborsRegressor()
    elif name=='GaussianProcessRegressor':
        model = GaussianProcessRegressor()    
    # Tree-based regressor
    elif name=='XGBRegressor':
        model = XGBRegressor()
    return model
#%%
# Kernel initialization
def restart_kernels(init_length_scale=1.0):
    """Function that calls kernels every time they need to be instanciated."""
    kernels = [1.0*RBF(length_scale=init_length_scale)+1.0*WhiteKernel(),
                1.0*DotProduct()+1.0*WhiteKernel(), 
                1.0*Matern(length_scale=init_length_scale, nu=0.5)+1.0*WhiteKernel(),
                1.0*Matern(length_scale=init_length_scale, nu=1.5)+1.0*WhiteKernel(),
                1.0*Matern(length_scale=init_length_scale, nu=2.5)+1.0*WhiteKernel(),  
                1.0*RationalQuadratic()+1.0*WhiteKernel()]
    return kernels
#%%
# Training loop for best models
if TRAINING_OPTION == 'full':
    best_models_names = ['GaussianProcessRegressor','KNeighborsRegressor','XGBRegressor']
elif TRAINING_OPTION == 'partial':
    best_models_names = ['GaussianProcessRegressor','GaussianProcessRegressor','XGBRegressor']

best_trained_models = []
for j,target in enumerate(TARGETS):
    y_train = Y_train[target]
    # type of regressor
    m_i = best_models_names[j]
    # define the model
    model = set_model(m_i)
    if m_i=='GaussianProcessRegressor':
        models[m_i]['kernel'] = restart_kernels(np.ones(X_train.shape[1]))
    # define search space
    space = models[m_i]
    # configure the cross-validation procedure
    cv_inner = KFold(n_splits=10, shuffle=True, random_state=SEED)
    # define search
    search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True, n_jobs=-1)
    # execute search
    result = search.fit(X_train, y_train)
    # get the best performing model fit on the whole training set
    best_model = result.best_estimator_
    best_trained_models.append(best_model)
#%%
"""# 3. Saving models"""

models_name = TRAINING_OPTION+'_dataset'+'_models.dump'
with open(models_name , "wb") as f:
    pickle.dump(best_trained_models, f)

params_name = 'standardizer_'+TRAINING_OPTION+'_dataset'+'.dump'
# Open a file and use dump()
with open(params_name, 'wb') as file:
    # A new file will be created
    pickle.dump([mu,sigma], file)

